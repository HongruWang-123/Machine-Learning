# -*- coding: utf-8 -*-
"""dataset1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egGEfkqgJN0F2P6Z9tidDjIIkjRnAEzZ

# Step 0: Download and Procress Data

You need to run the code below here and ensure that it works.  Assuming it has run without any error messages, there is nothing more you need to do.

## Fetch Data from UCI

This downloads the data from the UCI machine learning database repository.  If are using this notebook on a local machine and it doesn't work for you, you can download the files directly from https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/ and place them in your working directory.
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names

print("First lines of the datafile:")
!head -n 10 arrhythmia.names

print("First lines of the datafile:")
!head -n 5 arrhythmia.data

"""## Load and Parse Data with Pandas

This sets up a Pandas DataFrame object which has proper names and datatypes for the dataset.  Pandas is a standard and widely used library for data science.  You can learn more about it here: https://pandas.pydata.org/docs/getting_started/index.html
"""

import pandas as pd
import numpy as np
import scipy as sp
import scipy.special as spsp
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning) 

channels = ['di','dii','diii','avr','avl','avf','v1','v2','v3','v4','v5','v6']
features = [('age','num'), ('sex','bool'), ('height','num'), ('weight','num')] + \
           [('qrs_dur','num'), ('pr_inter','num'), ('qt_inter','num'), ('t_inter','num'), ('p_inter','num')] + \
           [('qrs','num'), ('t','num'), ('p','num'), ('qrst','num'), ('j','num'), ('heart_rate','num')]
for chan in channels:
  for fn in ['width_q', 'width_r', 'width_s', 'width_rp', 'width_sp','num_deflections',
             'ragged_r', 'diphasic_r','ragged_p', 'diphasic_p',
             'ragged_t', 'diphasic_t']:

    if fn.startswith('ragged') or fn.startswith('diphasic'):
      features.append((chan + '_' + fn,'bool'))
    else:
      features.append((chan + '_' + fn,'num'))

for chan in channels:
  for fn in ['amp_jj', 'amp_q', 'amp_r', 'amp_s', 'amp_rp', 'amp_sp', 'amp_p', 'amp_t', 'qrsa', 'qrsta']:
    features.append((chan + '_' + fn,'num'))
    # feature_names.append(chan + '_' + fn)

features.append(('class','cat'))
feature_names = [ a for a,b in features ]
dtype_map = {'bool':np.bool, 'num':np.float64, 'cat':np.int8}
feature_dtypes = dict([(a,dtype_map[b]) for a,b in features ])

data = pd.read_csv('arrhythmia.data',
                   names=feature_names,dtype=feature_dtypes,
                   na_values='?')
data.head()

"""## Remove columns for features with missing values
Missing values occur frequently in real data such as this.  For now we will remove input columns which have missing data but handling them properly can be critical in practice.
"""

missing_cols = []
for c in data.columns:
  if np.any(pd.isnull(data[c])):
    missing_cols.append(c)

print("Removing " + str(missing_cols))
clean_data = data.drop(columns=missing_cols)
num_data = len(clean_data)
print("There are {} input dimensions".format(clean_data.shape[1]-1))

"""# Step 1: Data Exploration
A critical step in any ML application is dataset exploration.  You should always be familiar with your data.  How many inputs do you have?  What kinds are they?  If you do simple visualizations, can you get a sense of whether the prediction problem is possible.

## Produce histograms
Produce histograms of some of the inputs to visualize the distribution of different input dimensions and their potential impact on the class label.
"""

vis_dims = ['age','sex','height','weight','qrs_dur','pr_inter','qt_inter','t_inter','p_inter']

for c in vis_dims:
  plt.figure()
  coltype = clean_data.dtypes[c]
  coldata = clean_data[c].astype(np.float32)
  plt.title('column {}'.format(c))
  plt.hist([coldata[clean_data['class'] == 1],
            coldata[clean_data['class'] != 1]],
           density=True, # normalize the histograms
           label=['Normal','Other'])
  plt.legend()

"""## Split data into training and validation
Randomly select a subset of the data to use for training and the rest for validation.  For educational purposes we are going to use a small fraction of the dataset for training (5%) in order to more easily see the value of regularization.  In a real world setting you would typically use something like 80% or 90% for training.
"""

def split_dataset(input_data,pct_train,seed=0):
  num_data = len(input_data)
  # Set a fixed random seed to ensure repeatability
  np.random.seed(seed)
  perm = np.random.permutation(num_data)

  num_train = int(pct_train*num_data)
  num_val = num_data - num_train

  data_train = input_data.iloc[perm[0:num_train]].reset_index(drop=True)
  data_val = input_data.iloc[perm[num_train:]].reset_index(drop=True)

  print("Using {} samples for training data and {} samples for validation".format(num_train,num_val))

  # Split output from inputs
  train_x = data_train.drop(columns=['class'])
  train_y = data_train['class'] == 1 # Classify normal vs abnormal

  val_x = data_val.drop(columns=['class'])
  val_y = data_val['class'] == 1 # Classify normal vs abnormal

  return train_x, train_y, val_x, val_y

data_train_x, data_train_y, data_val_x, data_val_y = split_dataset(clean_data,pct_train = 0.05)

"""## Compute and print basic statistics

**YOU MUST IMPLEMENT AND ANSWER THINGS HERE**

Write code which computes and prints:
1. The fraction of training data for people under the age of 20 
2. The fraction of training data for people over the age of 50
3. The fraction of training data which are indicated as normal (i.e., with the value in the 'class' column being 1)

Answer the following questions in your report:
1. Does this seem like a good distribution of ages for the dataset?  Would you feel comfortable relying on the output of the resulting model if you were under 20?  What if you were over 50?  Why?
2. If you were to build a model which always spit out the same label (either normal or not), based on this training data what label should it output?  What do you think the accuracy of that model would be?  Why?
3. Based on the histograms above, how hard do you think this prediction task is and why?  Should we expect to get perfect?  Should we be able to do better than chance?  Include copies of any figures you reference in your answer to this question.


"""

num_train = data_train_x.shape[0]

frac_u20 = np.sum(data_train_x['age'] < 20)/num_train
print('Fraction of dataset with age < 20: {}'.format(frac_u20))

frac_o50 = np.sum(data_train_x['age'] > 50)/num_train
print('Fraction of dataset with age < 50: {}'.format(frac_o50))

frac_female = np.sum(data_train_x['sex'] == 1)/num_train
print('Fraction of dataset indicated as female: {}'.format(frac_female))

frac_normal = np.sum(data_train_y == 1)/num_train
print('Fraction of dataset indicated as normal record: {}'.format(frac_normal))

"""# Step 2: Implement a Naive Bayes classifier

## Code for a Naive Bayes model

This is the part which implements the mathematical model for our dataset.  For learning, this involves estimating the parameters of the appropriate distributions for each feature conditional on the output class label.  In order to make predictions, we also need to be able to evaluate the (log) probability of each distribution.

**YOU MUST IMPLEMENT THINGS HERE**
"""

def bernoulli_logpdf(x,theta):
  lp = np.zeros_like(x,dtype=np.float64)
  if theta == 0:
    lp[x==True] = -np.inf
  elif theta == 1:
    lp[x==False] = -np.inf
  else:
    lp[x==True] = np.log(theta)
    lp[x==False] = np.log1p(-theta)
  return lp

def gaussian_logpdf(x,mu,sigma):
  assert np.isfinite(mu)
  assert np.all(np.isfinite(x))
  sigma = np.maximum(1e-16,sigma) # Avoid having to handle the sigma=0 case

  # TODO: Implement the logpdf of a Gaussian distribution
  
  lp = np.log(1/(sigma*np.sqrt(2*np.pi))) - 0.5*((x-mu)/sigma)**2

  return lp

def fit_bernoulli(x,prior_alpha):
  # TODO: Compute MAP estimate of theta
  lp = np.zeros_like(x,dtype=np.float64)
  theta = (len(lp[x==True])+prior_alpha)/(len(x)+2*prior_alpha)
  return {'logprob':bernoulli_logpdf, 'params':{'theta':theta}}

def fit_gaussian(x,prior_mu_val,prior_sigma_val,prior_dof):
  numx = len(x)
  assert numx>2
  # TODO: Compute MAP estimate of mu and sigma
  mu = numx*np.mean(x)/(numx+prior_dof)
  sigma = np.sqrt((numx*np.power(np.std(x),2)+prior_dof)/(numx+prior_dof))
  return {'logprob':gaussian_logpdf, 'params':{'mu':mu,'sigma':sigma} }

def fit_naivebayes(x,y,prior_dof=0,prior_mu_val=0,prior_sigma_val=1):
  y_vals = [False, True]
  prior_dist = fit_bernoulli(y,prior_alpha=prior_dof)
  cond_dists = dict([ (yv,{}) for yv in y_vals ])
  for c in x.columns:
    coltype = x.dtypes[c]
    coldata = x[c]
    x_y0 = coldata[y == False]
    x_y1 = coldata[y == True]
    if coltype == 'float64' or coltype == 'float32':
      # fit gaussian
      fit_func = lambda cx: fit_gaussian(cx,prior_mu_val,prior_sigma_val,prior_dof)
    elif coltype == 'bool':
      # fit bernoulli
      fit_func = lambda cx: fit_bernoulli(cx,prior_alpha=prior_dof)
    for yv in y_vals:
      cond_dists[yv][c] = fit_func(coldata[y == yv])

  model = { 'prior':prior_dist,
            'conds':cond_dists,
            'y_vals':y_vals }
  return model

def eval_naivebayes(nbmodel,x,normalize=True):
  numx = x.shape[0]
  numyv = len(nbmodel['y_vals'])
  yv_lp = {}
  yv_all_lps = np.zeros((numyv,numx))
  for i,yv in enumerate(nbmodel['y_vals']):
    yv_lp[yv] = np.zeros(numx)
    yv_lp[yv] += nbmodel['prior']['logprob'](yv,**nbmodel['prior']['params'])
    for c,m in nbmodel['conds'][yv].items():
      clp = m['logprob'](x[c],**m['params'])
      yv_lp[yv] += clp
    yv_all_lps[i,:] = yv_lp[yv]

  if normalize:
    yv_lp_nrm = spsp.logsumexp(yv_all_lps,axis=0)
    ninf_yv_lp_nrm = np.isinf(yv_lp_nrm)

    for yv in nbmodel['y_vals']:
      curr_false = np.logical_and(np.isinf(yv_lp[yv]),ninf_yv_lp_nrm)
      yv_lp[yv] -= yv_lp_nrm
      yv_lp[yv][curr_false] = -np.inf

  return pd.DataFrame(yv_lp)

# Compare the predicted log probability values (yv_lp_xs) against their true labels (gt_ys)
def compare_predictions(gt_ys,yv_lp):
  assert yv_lp.shape[1] == 2 # this code only works for binary models
  
  # Lookup the log probability of the correct class
  correct_lps = yv_lp.lookup(range(len(gt_ys)),gt_ys)
 
  # TODO: Implement the calculations for these values which are used to compute stats

  # TODO: Total number of ys in the evaluation set
  num_ys = len(gt_ys)

  # TODO: Number of ys with the correct label having highest probability
  n = 0
  for a in range(len(gt_ys)):
    if gt_ys[a] == True and yv_lp[True][a]>yv_lp[False][a]:
      n+=1
    if gt_ys[a] == False and yv_lp[False][a]>yv_lp[True][a]:
      n+=1
  num_correct = n

  # TODO: Number of ys where y == 1
  length = 0
  for c in range(len(gt_ys)):
    if gt_ys[c] == True:
      length += 1
  
  num_pos_ys = length

  # TODO: Number of ys where y == 0
  length = 0
  for d in range(len(gt_ys)):
    if gt_ys[d] == False:
      length += 1

  num_neg_ys = length

  # TODO: Number of ys where y == 1 with the correct label having highest probability
  num = 0
  for b in range(len(gt_ys)):
    if gt_ys[b] == True and yv_lp[True][b]>yv_lp[False][b]:
      num += 1
 
  num_true_pos = num

  # TODO: Number of ys where y == 0 with the correct label having highest probability
  num = 0
  for e in range(len(gt_ys)):
    if gt_ys[e] == False and yv_lp[False][e]>yv_lp[True][e]:
      num += 1
 

  num_true_neg = num

  # TODO: Average log probability of the correct class
  total = 0
  for i in range(len(gt_ys)):
    if gt_ys[i] == True and yv_lp[True][i]>yv_lp[False][i]:
      total += yv_lp[True][i]
    if gt_ys[i] == False and yv_lp[False][i]>yv_lp[True][i]:
      total += yv_lp[False][i]
  
  avg_logprob = total/num_ys

  stats = {}
  stats['true_pos_rate'] = num_true_pos/num_pos_ys
  stats['true_neg_rate'] = num_true_neg/num_neg_ys
  stats['correct_lps'] = correct_lps
  stats['pct_correct'] = num_correct / num_ys
  stats['avg_logprob'] = avg_logprob
  return stats

"""## Fit the parameters of Naive Bayes with maximum likelihood and maximum a posteriori estimates

See what happens when using maximum likelihood estimates (i.e., `prior_dof=0`).
"""

nbmodel_ML = fit_naivebayes(data_train_x,data_train_y,prior_dof=0)

"""### Evaluate the ML-fit model on the training data"""

data_train_lp = eval_naivebayes(nbmodel_ML,data_train_x)
train_stats = compare_predictions(data_train_y,data_train_lp)

plt.figure()
plt.title('Training Set Probability of Correct Class')
plt.hist(np.exp(train_stats['correct_lps']))
print('Training Set Prediction Statistics')
print('  Accuracy: {}'.format(train_stats['pct_correct']))
print('  True Positive Rate: {}'.format(train_stats['true_pos_rate']))
print('  True Negative Rate: {}'.format(train_stats['true_neg_rate']))

"""### Evaluate the ML-fit model on the validation set"""

data_val_lp = eval_naivebayes(nbmodel_ML,data_val_x)
val_stats = compare_predictions(data_val_y,data_val_lp)

plt.figure()
plt.title('Validation Set Probability of Correct Class')
plt.hist(np.exp(val_stats['correct_lps']))
print('Validation Set Prediction Statistics')
print('  Accuracy: {}'.format(val_stats['pct_correct']))
print('  True Positive Rate: {}'.format(val_stats['true_pos_rate']))
print('  True Negative Rate: {}'.format(val_stats['true_neg_rate']))

"""## Explore different prior strengths

Fit Naive Bayes models with different values of `prior_dof`.  For each value, fit a model and evaluate it on both the training and validation set


**YOU NEED TO IMPLEMENT THINGS HERE**
"""

dofs = [0,0.1,0.25,0.5,0.75,1] + list(range(2,9,2)) + [ 2**d for d in range(4,5) ]
print(dofs)
print('Fitting models...',end="")
# TODO: Implement this
all_nbmodels = []
for index in range(len(dofs)):
  nbmodel_x = fit_naivebayes(data_train_x,data_train_y,prior_dof=dofs[index])
  all_nbmodels.append(nbmodel_x)
print('done.')

print('Evaluating models...',end="")
# TODO: Implement this
all_train_stats = []

for index1 in range(len(all_nbmodels)):
  data_train_lp = eval_naivebayes(all_nbmodels[index1],data_train_x)
  train_stats = compare_predictions(data_train_y,data_train_lp)
  all_train_stats.append(train_stats)


all_val_stats = []

for index2 in range(len(all_nbmodels)):
  data_val_lp = eval_naivebayes(all_nbmodels[index2],data_val_x)
  val_stats = compare_predictions(data_val_y,data_val_lp)
  all_val_stats.append(val_stats)

print('done.')

all_val_train_acc = [ stats['pct_correct'] for stats in all_train_stats ]
all_val_val_acc = [ stats['pct_correct'] for stats in all_val_stats ]

plt.figure()
plt.plot(dofs,all_val_train_acc,label='Train')
plt.plot(dofs,all_val_val_acc,label='Val')
plt.legend()

# TODO: select the model with the best performance on the validation set
max = 0
index = 0
for index3 in range(len(all_val_val_acc)):
  if all_val_val_acc[index3] >= max:
    index = index3
    max = all_val_val_acc[index3]

nbmodel_MAP = all_nbmodels[index]

"""**YOU NEED TO ANSWER THINGS HERE**

Consider the outputs above as you answer the following questions.
1. Describe and explain the difference between the model performance on the training and validation sets when performing maximum likelihood estimation.  That is, what differences do you notice in the performance and why they might be occuring?
2. Which value of `prior_dof` should you use if you only looked at the model performance on the training curve?  What about if you looked instead at the model performance on the validation curve?  Explain your answer.
3. What happens to model performance as `prior_dof` increases?  Why?  What would you expect to happen as `prior_dof` goes to infinity and why?  Specifically, how would the curves for training and validation performance behave in that limit?

# Step 3: Inspect the model

**YOU MUST IMPLEMENT THINGS HERE**
"""

def find_val_importance(nbmodel,x,y):
  imp_vals = []
  for c in x.columns:
    coltype = x.dtypes[c]
    if coltype == 'bool':
      # TODO: Compute importance value and append to imp_vals
      imp_val = abs(np.log(nbmodel['conds'][True][c]['params']['theta'])-np.log(nbmodel['conds'][False][c]['params']['theta']))
      imp_vals.append((c,imp_val))
  # sort imp_vals so that largest features are first
  imp_vals.sort(key=lambda x: x[1],reverse=True)
  return imp_vals

impvals_full = find_val_importance(nbmodel_MAP,data_train_x,data_train_y)
print('Top 10 most important binary features')
for k,v in impvals_full[:10]:
  print(' Importance of {}: {}'.format(k,v))

print('\nBottom 10 least important binary features')
for k,v in impvals_full[-10:]:
  print(' Importance of {}: {}'.format(k,v))